{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "light-underground",
   "metadata": {},
   "source": [
    "### Feature engineering techniques\n",
    "### List of Techniques\n",
    "<ol>\n",
    "    <li> Imputation </li>\n",
    "    <li> Handling Outliers </li>\n",
    "    <li> Binning </li>\n",
    "    <li> Log Transform </li>\n",
    "    <li> One-Hot Encoding </li>\n",
    "    <li> Grouping Operations </li>\n",
    "    <li> Feature Split </li>\n",
    "    <li> Scaling </li>\n",
    "    <li> Extracting Date </li>\n",
    "</ol>\n",
    "\n",
    "<a>https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vocational-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "confidential-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "suburban-groove",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sticky-representation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n",
      "PassengerId\n",
      "PassengerId    0.000000\n",
      "Survived       0.000000\n",
      "Pclass         0.000000\n",
      "Name           0.000000\n",
      "Sex            0.000000\n",
      "Age            0.198653\n",
      "SibSp          0.000000\n",
      "Parch          0.000000\n",
      "Ticket         0.000000\n",
      "Fare           0.000000\n",
      "Cabin          0.771044\n",
      "Embarked       0.002245\n",
      "dtype: float64\n",
      "PassengerId     True\n",
      "Survived        True\n",
      "Pclass          True\n",
      "Name            True\n",
      "Sex             True\n",
      "Age            False\n",
      "SibSp           True\n",
      "Parch           True\n",
      "Ticket          True\n",
      "Fare            True\n",
      "Cabin          False\n",
      "Embarked       False\n",
      "dtype: bool\n",
      "0      0.083333\n",
      "1      0.000000\n",
      "2      0.083333\n",
      "3      0.000000\n",
      "4      0.083333\n",
      "         ...   \n",
      "886    0.083333\n",
      "887    0.000000\n",
      "888    0.166667\n",
      "889    0.000000\n",
      "890    0.083333\n",
      "Length: 891, dtype: float64\n",
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'SibSp', 'Parch',\n",
      "       'Ticket', 'Fare'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(df.columns[0])\n",
    "\n",
    "threshold = 0.0\n",
    "# df.columns\n",
    "print(df.isnull().mean())\n",
    "print(df.isnull().mean() == threshold)\n",
    "\n",
    "print(df.isnull().mean(axis=1))\n",
    "\n",
    "print(df.columns[df.isnull().mean() == threshold])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-suite",
   "metadata": {},
   "source": [
    "# 1. Imputation\n",
    "\n",
    "<p>Missing values are one of the most common problems you can encounter when you try to prepare your data for machine learning. The reason for the missing values might be human errors, interruptions in the data flow, privacy concerns, and so on. Whatever is the reason, missing values affect the performance of the machine learning models.\n",
    "Some machine learning platforms automatically drop the rows which include missing values in the model training phase and it decreases the model performance because of the reduced training size. On the other hand, most of the algorithms do not accept datasets with missing values and gives an error.</p>\n",
    "\n",
    "<p>The most simple solution to the missing values is to drop the rows or the entire column. There is not an optimum threshold for dropping but you can use 70% as an example value and try to drop the rows and columns which have missing values with higher than this threshold.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "static-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 11)\n",
      "(891, 11)\n"
     ]
    }
   ],
   "source": [
    "### Dropping colums or rows.. \n",
    "\n",
    "threshold = 0.7\n",
    "#Dropping columns with missing value rate higher than threshold\n",
    "df = df[df.columns[df.isnull().mean() < threshold]]\n",
    "print(df.shape)\n",
    "\n",
    "#Dropping rows with missing value rate higher than threshold\n",
    "df = df.loc[df.isnull().mean(axis=1) < threshold]\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fundamental-feeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId    446.000000\n",
      "Survived         0.383838\n",
      "Pclass           2.308642\n",
      "Age             29.699118\n",
      "SibSp            0.523008\n",
      "Parch            0.381594\n",
      "Fare            32.204208\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#### IMPULATION\n",
    "print(df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-tennis",
   "metadata": {},
   "source": [
    "## Numerical Imputation\n",
    "best imputation way is to use the medians of the columns. As the averages of the columns are sensitive to the outlier values, while medians are more solid in this respect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "level-ensemble",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female  28.0      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Embarked  \n",
       "0        0         A/5 21171   7.2500        S  \n",
       "1        0          PC 17599  71.2833        C  \n",
       "2        0  STON/O2. 3101282   7.9250        S  \n",
       "3        0            113803  53.1000        S  \n",
       "4        0            373450   8.0500        S  \n",
       "..     ...               ...      ...      ...  \n",
       "886      0            211536  13.0000        S  \n",
       "887      0            112053  30.0000        S  \n",
       "888      2        W./C. 6607  23.4500        S  \n",
       "889      0            111369  30.0000        C  \n",
       "890      0            370376   7.7500        Q  \n",
       "\n",
       "[891 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numerical Imputation\n",
    "\n",
    "# Filling all missing values with 0\n",
    "df.fillna(0)\n",
    "\n",
    "# Filling all missing values with median of columns\n",
    "df.fillna(df.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-commercial",
   "metadata": {},
   "source": [
    "## Categorical Imputation\n",
    "Replacing the missing values with the maximum occurred value in a column is a good option for handling categorical columns. But if you think the values in the column are distributed uniformly and there is not a dominant value, imputing a category like “Other” might be more sensible, because in such a case, your imputation is likely to converge a random selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fourth-islam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        1\n",
      "1        2\n",
      "2        3\n",
      "3        4\n",
      "4        5\n",
      "      ... \n",
      "886    887\n",
      "887    888\n",
      "888    889\n",
      "889    890\n",
      "890    891\n",
      "Name: PassengerId, Length: 891, dtype: int64\n",
      "Index(['Name', 'Sex', 'Ticket', 'Embarked'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df['PassengerId'])\n",
    "df._get_numeric_data().columns\n",
    "category_colms  = df.select_dtypes(include=['object'])\n",
    "print(category_colms.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "focused-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max fill function for categorical columns\n",
    "category_colms  = df.select_dtypes(include=['object'])\n",
    "\n",
    "for each_cat_feature in category_colms.columns:\n",
    "    if each_cat_feature != 'Name':\n",
    "        df[each_cat_feature].fillna(df[each_cat_feature].value_counts().idxmax(), inplace=True)\n",
    "    else:\n",
    "        df[each_cat_feature].fillna('OTHERS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-navigator",
   "metadata": {},
   "source": [
    "# 2. Handling Outliers\n",
    "<p>Before mentioning how outliers can be handled, I want to state that the best way to detect the outliers is to demonstrate the data visually. All other statistical methodologies are open to making mistakes, whereas visualizing the outliers gives a chance to take a decision with high precision. Anyway, I am planning to focus visualization deeply in another article and let’s continue with statistical methodologies.</p>\n",
    "\n",
    "<p>Statistical methodologies are less precise as I mentioned, but on the other hand, they have a superiority, they are fast. Here I will list two different ways of handling outliers. These will detect them using standard deviation, and percentiles.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abroad-publicity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3838383838383838"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Survived'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-chassis",
   "metadata": {},
   "source": [
    "## Outlier Detection with Standard Deviation\n",
    "If a value has a distance to the average higher than x * standard deviation, it can be assumed as an outlier. Then what x should be?\n",
    "There is no trivial solution for x, but usually, a value between 2 and 4 seems practical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "chronic-locator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(657, 11)\n"
     ]
    }
   ],
   "source": [
    "#Dropping the outlier rows with standard deviation\n",
    "factor = 3\n",
    "\n",
    "numeric_colms = df._get_numeric_data().columns\n",
    "\n",
    "for each_numeric_feature in numeric_colms:\n",
    "    upper_lim = df[each_numeric_feature].mean() + df[each_numeric_feature].std() * factor\n",
    "    lower_lim = df[each_numeric_feature].mean() - df[each_numeric_feature].std() * factor\n",
    "    df = df[(df[each_numeric_feature] < upper_lim) & (df[each_numeric_feature] > lower_lim)]\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-special",
   "metadata": {},
   "source": [
    "In addition, z-score can be used instead of the formula above. Z-score (or standard score) standardizes the distance between a value and the mean using the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-transfer",
   "metadata": {},
   "source": [
    "## Outlier Detection with Percentiles\n",
    "Another mathematical method to detect outliers is to use percentiles. You can assume a certain percent of the value from the top or the bottom as an outlier. The key point is here to set the percentage value once again, and this depends on the distribution of your data as mentioned earlier.\n",
    "\n",
    "Additionally, a common mistake is using the percentiles according to the range of the data. In other words, if your data ranges from 0 to 100, your top 5% is not the values between 96 and 100. Top 5% means here the values that are out of the 95th percentile of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "minus-extent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8999999999999995\n",
      "1.0\n",
      "0     True\n",
      "1     True\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "Name: Val, dtype: bool\n",
      "4.1\n",
      "3    4.1\n",
      "4    5.1\n",
      "Name: Val, dtype: float64\n",
      "0    1.1\n",
      "1    2.1\n",
      "Name: Val, dtype: float64\n",
      "0    1.1\n",
      "3    4.1\n",
      "4    5.1\n",
      "Name: Val, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame({\"Val\" : [1.1,2.1,3.1,4.1,5.1]})\n",
    "print(data['Val'].quantile(0.95))\n",
    "\n",
    "print(df['Survived'].quantile(0.95))\n",
    "\n",
    "print(data['Val'] < 3)\n",
    "\n",
    "print(data.loc[3, 'Val'])\n",
    "print(data.loc[[3,4], 'Val'])\n",
    "\n",
    "print(data.loc[data['Val'] < 3, 'Val'])\n",
    "\n",
    "print(data.loc[[True, False, False, True, True],'Val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "radio-fountain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(523, 11)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping the outlier rows with Percentiles\n",
    "\n",
    "numeric_colms = df._get_numeric_data().columns\n",
    "\n",
    "for each_numeric_feature in numeric_colms:\n",
    "    upper_lim = df[each_numeric_feature].quantile(.95)\n",
    "    lower_lim = df[each_numeric_feature].quantile(.05)\n",
    "    if each_numeric_feature == 'Age' or each_numeric_feature == 'Fare':\n",
    "        df = df[(df[each_numeric_feature] < upper_lim) & (df[each_numeric_feature] > lower_lim)]\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-purple",
   "metadata": {},
   "source": [
    "## An Outlier Dilemma: Drop or Cap\n",
    "Another option for handling outliers is to cap them instead of dropping. So you can keep your data size and at the end of the day, it might be better for the final model performance.\n",
    "\n",
    "On the other hand, capping can affect the distribution of the data, thus it better not to exaggerate it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "stainless-immigration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(523, 11)\n"
     ]
    }
   ],
   "source": [
    "# Capping the outlier rows with Percentiles\n",
    "\n",
    "numeric_colms = df._get_numeric_data().columns\n",
    "\n",
    "for each_numeric_feature in numeric_colms:\n",
    "    upper_lim = df[each_numeric_feature].quantile(.95)\n",
    "    lower_lim = df[each_numeric_feature].quantile(.05)\n",
    "    df.loc[(df[each_numeric_feature] > upper_lim),each_numeric_feature] = upper_lim\n",
    "    df.loc[(df[each_numeric_feature] < lower_lim),each_numeric_feature] = lower_lim\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-longer",
   "metadata": {},
   "source": [
    "## 3. Binning\n",
    "<img src=\"https://miro.medium.com/max/700/0*XWta_U67Nv9udfY-.png\" height=\"150px;\" width=\"400px;\">\n",
    "<p style=\"text-align: center;\">Binning illustration of numerical data</p>\n",
    "\n",
    "Binning can be applied on both categorical and numerical data:\n",
    "\n",
    "<div>\n",
    "#Numerical Binning Example <br>\n",
    "Value      Bin     <br>  \n",
    "0-30   ->  Low     <br>\n",
    "31-70  ->  Mid     <br>\n",
    "71-100 ->  High    <br>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "#Categorical Binning Example\n",
    "Value      Bin           <br>\n",
    "Spain  ->  Europe        <br>\n",
    "Italy  ->  Europe        <br>\n",
    "Chile  ->  South America <br>\n",
    "Brazil ->  South America <br>\n",
    "</div>\n",
    "\n",
    "The main motivation of binning is to make the model more robust and prevent overfitting, however, it has a cost to the performance. Every time you bin something, you sacrifice information and make your data more regularized.\n",
    "\n",
    "The trade-off between performance and overfitting is the key point of the binning process. In my opinion, for numerical columns, except for some obvious overfitting cases, binning might be redundant for some kind of algorithms, due to its effect on model performance.\n",
    "\n",
    "However, for categorical columns, the labels with low frequencies probably affect the robustness of statistical models negatively. Thus, assigning a general category to these less frequent values helps to keep the robustness of the model. For example, if your data size is 100,000 rows, it might be a good option to unite the labels with a count less than 100 to a new category like <b>“Other”</b>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-capture",
   "metadata": {},
   "source": [
    "### # Numerical Binning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dominant-wonder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1   Feature2\n",
      "0         2      Spain\n",
      "1        45      Chile\n",
      "2         7  Australia\n",
      "3        85      Italy\n",
      "4        28     Brazil\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame({'Feature1':[2,45,7,85,28], 'Feature2': ['Spain','Chile','Australia','Italy','Brazil']})\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sweet-jenny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1   Feature2   bin\n",
      "0         2      Spain   Low\n",
      "1        45      Chile   Mid\n",
      "2         7  Australia   Low\n",
      "3        85      Italy  High\n",
      "4        28     Brazil   Low\n"
     ]
    }
   ],
   "source": [
    "data['bin'] = pd.cut(data['Feature1'], bins=[0,30,70,100], labels=[\"Low\", \"Mid\", \"High\"])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-lyric",
   "metadata": {},
   "source": [
    "### # Categorical Binning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "corresponding-bonus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1   Feature2   bin      Continent\n",
      "0         2      Spain   Low         Europe\n",
      "1        45      Chile   Mid  South America\n",
      "2         7  Australia   Low          Other\n",
      "3        85      Italy  High         Europe\n",
      "4        28     Brazil   Low  South America\n"
     ]
    }
   ],
   "source": [
    "conditions = [\n",
    "    data['Feature2'].str.contains('Spain'),\n",
    "    data['Feature2'].str.contains('Italy'),\n",
    "    data['Feature2'].str.contains('Chile'),\n",
    "    data['Feature2'].str.contains('Brazil')]\n",
    "\n",
    "choices = ['Europe', 'Europe', 'South America', 'South America']\n",
    "\n",
    "data['Continent'] = np.select(conditions, choices, default='Other')\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-newsletter",
   "metadata": {},
   "source": [
    "### ############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fifteen-collar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1.609438\n",
      "1     2.302585\n",
      "2     3.218876\n",
      "3     2.708050\n",
      "4     3.332205\n",
      "5     3.637586\n",
      "6     3.806662\n",
      "7     3.850148\n",
      "8     3.496508\n",
      "9    11.002100\n",
      "Name: A, dtype: float64\n",
      "0.0 1.0\n",
      "          A\n",
      "0 -0.334575\n",
      "1 -0.334297\n",
      "2 -0.333463\n",
      "3 -0.334019\n",
      "4 -0.333296\n",
      "5 -0.332740\n",
      "6 -0.332351\n",
      "7 -0.332240\n",
      "8 -0.333018\n",
      "9  2.999999\n"
     ]
    }
   ],
   "source": [
    "z_data = pd.DataFrame({'A':[5,10,25,15,28,38,45,47,33, 60000]})\n",
    "\n",
    "print(np.log(z_data['A']))\n",
    "\n",
    "from scipy.stats import zscore\n",
    "z_transformed_data = z_data.apply(zscore)\n",
    "\n",
    "from scipy.stats import norm\n",
    "mu, std = norm.fit(z_transformed_data)\n",
    "\n",
    "print(mu, std)\n",
    "print(z_transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "helpful-reserve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
      "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
      "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
      "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
      "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
      "\n",
      "   PTRATIO       B  LSTAT  Price  \n",
      "0     15.3  396.90   4.98   24.0  \n",
      "1     17.8  396.90   9.14   21.6  \n",
      "2     17.8  392.83   4.03   34.7  \n",
      "3     18.7  394.63   2.94   33.4  \n",
      "4     18.7  396.90   5.33   36.2  \n",
      "5.223148798243851\n",
      "14.33370    2\n",
      "0.01501     2\n",
      "0.08265     1\n",
      "0.53700     1\n",
      "1.35472     1\n",
      "           ..\n",
      "13.67810    1\n",
      "0.88125     1\n",
      "0.01951     1\n",
      "0.49298     1\n",
      "0.03578     1\n",
      "Name: CRIM, Length: 504, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f8ba373d4a8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import seaborn as sns\n",
    "\n",
    "boston = load_boston()\n",
    "x = boston['data']\n",
    "y = boston['target']\n",
    "col = boston['feature_names']\n",
    "\n",
    "data = pd.DataFrame(x, columns=col)\n",
    "data['Price'] = y\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "print(data['CRIM'].skew())\n",
    "\n",
    "print(data['CRIM'].value_counts())\n",
    "X=[i for i in range(80)]\n",
    "sns.relplot(data=data['CRIM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-headset",
   "metadata": {},
   "source": [
    "### ############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-source",
   "metadata": {},
   "source": [
    "## 4.Log Transform\n",
    "Logarithm transformation (or log transform) is one of the most commonly used mathematical transformations in feature engineering. What are the benefits of log transform:\n",
    "<ul>\n",
    "   <li> It helps to handle skewed data and after transformation, the distribution becomes more approximate to normal.\n",
    "   <li> In most of the cases the magnitude order of the data changes within the range of the data. For instance, the difference between ages 15 and 20 is not equal to the ages 65 and 70. In terms of years, yes, they are identical, but for all other aspects, 5 years of difference in young ages mean a higher magnitude difference. This type of data comes from a multiplicative process and log transform normalizes the magnitude differences like that. </li>\n",
    "   <li> It also decreases the effect of the outliers, due to the normalization of magnitude differences and the model become more robust.</li>\n",
    "</ul>\n",
    "\n",
    "<b>A critical note:</b> The data you apply log transform must have only positive values, otherwise you receive an error. Also, you can add 1 to your data before transform it. Thus, you ensure the output of the transformation to be positive.\n",
    "    \n",
    "<b>Log(x+1)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "charitable-clinic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   value  log(x+1)  log(x-min(x)+1)\n",
      "0      2  1.098612         3.258097\n",
      "1     45  3.828641         4.234107\n",
      "2    -23       NaN         0.000000\n",
      "3     85  4.454347         4.691348\n",
      "4     28  3.367296         3.951244\n",
      "5      2  1.098612         3.258097\n",
      "6     35  3.583519         4.077537\n",
      "7    -12       NaN         2.484907\n"
     ]
    }
   ],
   "source": [
    "# Log Transform Example\n",
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "\n",
    "data['log(x+1)'] = (data['value']+1).transform(np.log)\n",
    "\n",
    "# Handling negative values\n",
    "# Note that the values are different\n",
    "data['log(x-min(x)+1)'] = (data['value']-data['value'].min()+1).transform(np.log)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-attribute",
   "metadata": {},
   "source": [
    "## 5.One-hot encoding\n",
    "One-hot encoding is one of the most common encoding methods in machine learning. This method spreads the values in a column to multiple flag columns and assigns 0 or 1 to them. These binary values express the relationship between grouped and encoded column.\n",
    "\n",
    "This method changes your categorical data, which is challenging to understand for algorithms, to a numerical format and enables you to group your categorical data without losing any information\n",
    "\n",
    "<b>Why One-Hot?:</b> If you have N distinct values in the column, it is enough to map them to N-1 binary columns, because the missing value can be deducted from other columns. If all the columns in our hand are equal to 0, the missing value must be equal to 1. This is the reason why it is called as one-hot encoding. However, I will give an example using the get_dummies function of Pandas. This function maps all values in a column to multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adult-latin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "0  a  b\n",
      "1  b  a\n",
      "2  a  c\n",
      "   A  a  b  c\n",
      "0  a  0  1  0\n",
      "1  b  1  0  0\n",
      "2  a  0  0  1\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame({\n",
    "          'A':['a','b','a'],\n",
    "          'B':['b','a','c']\n",
    "        })\n",
    "\n",
    "print(data)\n",
    "\n",
    "# Get one hot encoding of columns B\n",
    "one_hot = pd.get_dummies(data['B'])\n",
    "\n",
    "# Drop column B as it is now encoded\n",
    "data = data.drop('B',axis = 1)\n",
    "\n",
    "# Join the encoded df\n",
    "data = data.join(one_hot)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-hawaii",
   "metadata": {},
   "source": [
    "### # One-hot encoding using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "atlantic-oxide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data = np.array([15,4,9,11])\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "one_hot_encoded = encoder.fit_transform(data.reshape(len(data),1))\n",
    "print(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "indoor-marsh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1 0 2]\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data = np.array(['Red', 'Green', 'Blue', 'Orange'])\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "\n",
    "label_encoded = label_enc.fit_transform(data)\n",
    "\n",
    "print(label_encoded)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "one_hot_encoded = encoder.fit_transform(label_encoded.reshape(len(data),1))\n",
    "print(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-eagle",
   "metadata": {},
   "source": [
    "## 6. Grouping Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-citation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "polished-serve",
   "metadata": {},
   "source": [
    "## 7. Feature Split\n",
    "\n",
    "Splitting features is a good way to make them useful in terms of machine learning. Most of the time the dataset contains string columns that violates tidy data principles. By extracting the utilizable parts of a column into new features:\n",
    "\n",
    "- We enable machine learning algorithms to comprehend them.\n",
    "- Make possible to bin and group them.\n",
    "- Improve model performance by uncovering potential information.\n",
    "\n",
    "<b>Split</b> function is a good option, however, there is no one way of splitting features. It depends on the characteristics of the column, how to split it. Let’s introduce it with two examples. First, a simple split function for an ordinary name column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "driven-scroll",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Name FirstName LastName\n",
      "0  first1 last1    first1    last1\n",
      "1  first2 last2    first2    last2\n",
      "2  first3 last3    first3    last3\n",
      "3  first4 last4    first4    last4\n",
      "4  first5 last5    first5    last5\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame({'Name': ['first1 last1', 'first2 last2', 'first3 last3', 'first4 last4', 'first5 last5']})\n",
    "\n",
    "# Extracting first names\n",
    "data['FirstName'] = data['Name'].str.split(' ').map(lambda x: x[0])\n",
    "\n",
    "# Extracting last name\n",
    "data['LastName'] = data['Name'].str.split(' ').map(lambda x: x[-1])\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-judges",
   "metadata": {},
   "source": [
    "The example above handles the names longer than two words by taking only the first and last elements and it makes the function robust for corner cases, which should be regarded when manipulating strings like that.\n",
    "\n",
    "Another case for split function is to extract a string part between two chars. The following example shows an implementation of this case by using two split functions in a row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "focused-median",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Title  Year\n",
      "0                    Toy Story (1995)  1995\n",
      "1                      Jumanji (1995)  1995\n",
      "2             Grumpier Old Men (1995)  1995\n",
      "3            Waiting to Exhale (1995)  1995\n",
      "4  Father of the Bride Part II (1995)  1995\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame({'Title': ['Toy Story (1995)', 'Jumanji (1995)', 'Grumpier Old Men (1995)', \\\n",
    "                               'Waiting to Exhale (1995)', 'Father of the Bride Part II (1995)']})\n",
    "\n",
    "# data['Year'] = data['Title'].str.split('(').map(lambda x: x[-1]).str.split(')').map(lambda x: x[0])\n",
    "# or\n",
    "data['Year'] = data['Title'].str.split(\"(\", n=1, expand=True)[1].str.split(\")\", n=1, expand=True)[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-andrew",
   "metadata": {},
   "source": [
    "## 8. Scaling\n",
    "\n",
    "In most cases, the numerical features of the dataset do not have a certain range and they differ from each other. In real life, it is nonsense to expect age and income columns to have the same range. But from the machine learning point of view, how these two columns can be compared?\n",
    "\n",
    "Scaling solves this problem. The continuous features become identical in terms of the range, after a scaling process. This process is not mandatory for many algorithms, but it might be still nice to apply. However, the algorithms based on distance calculations such as k-NN or k-Means need to have scaled continuous features as model input.\n",
    "\n",
    "Basically, there are two common ways of scaling:\n",
    "\n",
    "### (1) Normalization\n",
    "<img src=\"https://miro.medium.com/max/168/1*D3ORMiW9A7GoTezFYbL8LA.png\">\n",
    "\n",
    "Normalization (or min-max normalization) scale all values in a fixed range between 0 and 1. This transformation does not change the distribution of the feature and <b>due to the decreased standard deviations, the effects of the outliers increases. Therefore, before normalization, it is recommended to handle the outliers.</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "modern-domestic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Value  Normalized\n",
      "0      2    0.231481\n",
      "1     45    0.629630\n",
      "2    -23    0.000000\n",
      "3     85    1.000000\n",
      "4     28    0.472222\n",
      "5      2    0.231481\n",
      "6     35    0.537037\n",
      "7    -12    0.101852\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame({'Value':[2, 45, -23, 85, 28, 2, 35, -12]})\n",
    "\n",
    "data['Normalized'] = ( data['Value'] - data['Value'].min() ) / ( data['Value'].max() - data['Value'].min())\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-symbol",
   "metadata": {},
   "source": [
    "### (2) Standardization\n",
    "Standardization (or <b>z-score normalization</b>) scales the values while taking into account standard deviation. If the <b>standard deviation</b> of features is different, their range also would differ from each other. <b>This reduces the effect of the outliers in the features</b>.\n",
    "\n",
    "In the following formula of standardization, the mean is shown as <b>μ</b> and the standard deviation is shown as <b>σ</b>.\n",
    "<img src=\"https://miro.medium.com/max/82/1*BcNLM9loyAR3YQLt2hDqqg.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "temporal-illinois",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Value  Standardized\n",
      "0      2     -0.518878\n",
      "1     45      0.703684\n",
      "2    -23     -1.229670\n",
      "3     85      1.840952\n",
      "4     28      0.220346\n",
      "5      2     -0.518878\n",
      "6     35      0.419367\n",
      "7    -12     -0.916922\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame({'Value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "\n",
    "data['Standardized'] = ( data['Value'] - data['Value'].mean() ) / data['Value'].std()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-invalid",
   "metadata": {},
   "source": [
    "## 9. Extracting Date\n",
    "Though date columns usually provide valuable information about the model target, they are neglected as an input or used nonsensically for the machine learning algorithms. It might be the reason for this, that dates can be present in numerous formats, which make it hard to understand by algorithms, even they are simplified to a format like <b>\"01–01–2017\"</b>.\n",
    "\n",
    "Building an ordinal relationship between the values is very challenging for a machine learning algorithm if you leave the date columns without manipulation. Here, I suggest three types of preprocessing for dates:\n",
    "- Extracting the parts of the date into different columns: Year, month, day, etc.\n",
    "- Extracting the time period between the current date and columns in terms of years, months, days, etc.\n",
    "- Extracting some specific features from the date: Name of the weekday, Weekend or not, holiday or not, etc.\n",
    "\n",
    "If you transform the date column into the extracted columns like above, the information of them become disclosed and machine learning algorithms can easily understand them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fifteen-rugby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        DATE  Year  Month  Passed_Years  Passed_Months Weekday_Name\n",
      "0 2017-01-01  2017      1             4             49       Sunday\n",
      "1 2008-12-04  2008     12            13            146     Thursday\n",
      "2 1988-06-23  1988      6            33            392     Thursday\n",
      "3 1999-08-25  1999      8            22            258    Wednesday\n",
      "4 1993-02-20  1993      2            28            336     Saturday\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "data = pd.DataFrame({'DATE':\n",
    "['01-01-2017',\n",
    "'04-12-2008',\n",
    "'23-06-1988',\n",
    "'25-08-1999',\n",
    "'20-02-1993',\n",
    "]})\n",
    "\n",
    "# transform string to date\n",
    "data['DATE'] = pd.to_datetime(data.DATE, format=\"%d-%m-%Y\")\n",
    "\n",
    "# Extracting Year\n",
    "data['Year'] = data['DATE'].dt.year\n",
    "\n",
    "# Extracting Month\n",
    "data['Month'] = data.DATE.dt.month\n",
    "\n",
    "# Extracting passed years since the date\n",
    "data['Passed_Years'] = date.today().year - data['DATE'].dt.year\n",
    "\n",
    "# Extracting passed months since the date\n",
    "data['Passed_Months'] = (date.today().year - data['DATE'].dt.year)*12 + (date.today().month - data['DATE'].dt.month)\n",
    "\n",
    "# Extracting the weekday name of the date\n",
    "data['Weekday_Name'] = data['DATE'].dt.day_name()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-measurement",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We saw fundamental methods that can be beneficial in the feature engineering process. After this article, proceeding with other topics of data preparation such as <b>feature selection, train/test splitting, and sampling</b> might be a good option.\n",
    "\n",
    "These techniques are not magical tools. If your data tiny, dirty and useless, feature engineering may remain incapable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
